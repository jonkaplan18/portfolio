{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting winners \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "\n",
    "\n",
    "pd.options.display.max_columns=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read in the nominee reviews. These are all reviews for all nominees. Some end up winning, others do not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320535\n",
      "Index(['bookID', 'bookLink', 'bookTitle', 'authorName', 'award', 'loser',\n",
      "       'winner', 'user_id', 'book_id', 'review_id', 'rating', 'review_text',\n",
      "       'date_added', 'date_updated', 'read_at', 'started_at', 'n_votes',\n",
      "       'n_comments'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "awardReviews=pd.read_csv('awardReviews.csv')\n",
    "print(len(awardReviews))\n",
    "print(awardReviews.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    212341\n",
       "1    108194\n",
       "Name: winner, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awardReviews.winner.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About half of our nominee reviews are for winners. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "awardReviews['reviewDate'] = pd.to_datetime(awardReviews.date_added,utc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan \n",
    "\n",
    "We need to train a NLP model to classify the reviews. From there we will predict for each review whether we think it is for a winner or loser\n",
    "\n",
    "1. Transformer model\n",
    "2. aggregate\n",
    "3. traditional classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "awardTexts = awardReviews[['bookID','review_text','winner','n_votes']].rename(columns={'review_text':'text','winner':'labels'})\n",
    "\n",
    "awardTexts['text'] = awardTexts['text'].astype(str).str.replace('\\D+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214758 in train set, 105777 in test\n"
     ]
    }
   ],
   "source": [
    "#X = awardTexts.drop('labels',axis=1)\n",
    "#y = awardTexts.labels\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "#train = pd.concat([X_train, y_train], axis=1)\n",
    "#test = pd.concat([X_test,y_test],axis=1)\n",
    "#print('{} in train set, {} in test'.format(len(train),len(test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, validate, test = np.split(awardTexts.sample(frac=1), [int(.6*len(awardTexts)), int(.8*len(awardTexts))])\n",
    "#print('{} in train, {} in validate, {} in test'.format(len(train),len(validate),len(test)))\n",
    "train, test = np.split(awardTexts.sample(frac=1), [int(.8*len(awardTexts))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    170023\n",
       "0    170023\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# separate minority and majority classes\n",
    "lost = train[train.labels==0]\n",
    "won = train[train.labels==1]\n",
    "\n",
    "# upsample minority\n",
    "win_resampled = resample(won,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(lost), # match number in majority class\n",
    "                          random_state=42) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([lost,win_resampled])\n",
    "\n",
    "# check new class counts\n",
    "upsampled.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238032 train, 102014 validate, 64107 final test\n"
     ]
    }
   ],
   "source": [
    "train,validate = np.split(upsampled.sample(frac=1), [int(0.7*len(upsampled))])\n",
    "print('{} train, {} validate, {} final test'.format(len(train),len(validate),len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample from these to make it runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small = train.sample(n=100000)[['text','labels']]\n",
    "eval_df = validate.sample(n=10000)[['text','labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[['text']].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbafb1c576624c1c9e5eba210e75e1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a86e9331bbb40e4a2227f196f5704bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=5.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133d632ac58d473a9c1523f886bd303f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=12500.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.783035Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Running loss: 0.588914Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Running loss: 0.771424Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Running loss: 0.691651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optional model configuration\n",
    "model_args = {\n",
    "    \"num_train_epochs\": 5,\n",
    "    \"sliding_window\": True,\n",
    "    'use_early_stopping': True,\n",
    "    'early_stopping_patience': 3,\n",
    "    \"early_stopping_delta\": 0.01,\n",
    "    \"early_stopping_metric\": \"mcc\",\n",
    "    \"evaluate_during_training\":True,\n",
    "    \"evaluate_during_training_steps\": 1000,\n",
    "    \"save_model_every_epoch\": False,\n",
    "    'overwrite_output_dir': True,\n",
    "    'no_cache': True,\n",
    "    'output_dir':'transformers-outputs/winners',\n",
    "    #'weight':[0.5]\n",
    "}\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel(\n",
    "    \"electra\", \"google/electra-small-discriminator\", args=model_args\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_small,eval_df=eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test['text'] = test['text'].astype(str).str.replace('\\D+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60516d0d398741439afaacc68ee79d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=64107.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1f478b829c4f18bc741dd189f28ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8014.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'mcc': 0.0, 'tp': 0, 'tn': 42318, 'fp': 0, 'fn': 21789, 'acc': 0.6601151200336937, 'f1': 0.0, 'eval_loss': 0.6899995799891099}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result, model_outputs, wrong_predictions = model.eval_model(test[['text','labels']],acc=sklearn.metrics.accuracy_score,f1=sklearn.metrics.f1_score)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
